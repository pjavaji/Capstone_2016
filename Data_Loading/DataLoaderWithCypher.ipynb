{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# April 2016\n",
    "# Debbie Hofman\n",
    "# Load data from Twitter API into a Cypher graph similar to\n",
    "# https://gist.github.com/nicolewhite/fb41e76844ce10183849 oscon_twitter_graph.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Data Loader uses Cypher to do ELT process of transforming the raw JSON input into a neo4j Graph\n",
    "### Written for neo4j v. 2.3.3 using py2neo library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "#!wc -l \"./archive/data/election172\"\n",
    "!cat \"./archive/data/election172\" | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cypher_data_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cypher_data_loader.py\n",
    "\n",
    "import sys, getopt\n",
    "import py2neo as neo4j\n",
    "import json\n",
    "import datetime\n",
    "import math\n",
    "import time\n",
    "\n",
    "epoch = datetime.datetime.utcfromtimestamp(0)\n",
    "\n",
    "def readargs(argv):\n",
    "    \n",
    "    inputfile=''\n",
    "    password=''\n",
    "    remoteserver=''\n",
    "    maxsize=600000\n",
    "    batchsize=1000\n",
    "    partition_key='election'\n",
    "    constraints=False\n",
    "    partitionsize=0\n",
    "    startval=0\n",
    "    \n",
    "    try:\n",
    "        opts, args = getopt.getopt(argv,\"hi:p:r:m:b:k:z:s:c\")\n",
    "    except getopt.GetoptError:\n",
    "        print 'neo2.py -i <inputfile> -p <password for local server> -r <remote server url> \\\n",
    "        -m <maxnumberrows> -b <batchsize> -k <partition_key> -s <startval>'\n",
    "        sys.exit(2)\n",
    "    for opt, arg in opts:\n",
    "        if opt == '-h':\n",
    "            print 'neo2.py -i <inputfile> -p <password for local server> -r <remote server url> \\\n",
    "            -m <maxnumberrows> -b <batchsize> -k <partition_key> -s <startval>'\n",
    "            sys.exit()\n",
    "        elif opt == \"-i\":\n",
    "            inputfile = arg\n",
    "        elif opt == \"-p\":\n",
    "            password = arg\n",
    "        elif opt == \"-r\":\n",
    "            remoteserver = arg\n",
    "        elif opt == \"-m\":\n",
    "            maxsize = int(arg)\n",
    "        elif opt == \"-b\":\n",
    "            batchsize = int(arg)\n",
    "        elif opt == \"-k\":\n",
    "            partition_key = arg\n",
    "        elif opt == \"-c\":\n",
    "            constraints = True\n",
    "        elif opt == \"-z\":\n",
    "            partitionsize = arg\n",
    "        elif opt == \"-s\":\n",
    "            startval=int(arg)\n",
    "            \n",
    "    return inputfile, password, remoteserver, maxsize, batchsize, partition_key, constraints, partitionsize, startval\n",
    "\n",
    "def authenticate(password, remoteserver):\n",
    "    mode='none'\n",
    "    try:\n",
    "        if ((password=='') and (remoteserver=='')):\n",
    "            mode='no auth'\n",
    "            graph = neo4j.Graph()\n",
    "            return graph\n",
    "\n",
    "        elif (remoteserver==''):\n",
    "            mode='password local'\n",
    "            pyn.authenticate(\"localhost:7474\",\"neo4j\",password)\n",
    "            graph = neo4j.Graph()\n",
    "            return graph\n",
    "\n",
    "        elif (remoteserver!=''):\n",
    "            mode='remote server'\n",
    "            remote_graph = neo4j.Graph(remoteserver)\n",
    "            return remote_graph\n",
    "\n",
    "    except:\n",
    "        print 'neo2.py problem with authentication for mode ' + mode\n",
    "        sys.exit(2)\n",
    "    \n",
    "    return\n",
    "   \n",
    "def unix_time_millis(dt):\n",
    "    return int( (dt - epoch).total_seconds() * 1000.0)\n",
    "\n",
    "def addtweet(data,databatch):\n",
    "    \n",
    "    pretweet=''\n",
    "    \n",
    "    if (('retweeted' in data) and ('retweeted_status' in data)):\n",
    "        pretweet = data['retweeted_status'];\n",
    "\n",
    "    elif (('quoted_status' in data)):\n",
    "        pretweet = data['quoted_status'];\n",
    "        \n",
    "    if (pretweet != ''):\n",
    "        databatch['tweets'].append(pretweet) \n",
    "    \n",
    "    databatch['tweets'].append(data)\n",
    "\n",
    "#create roughly hourly partition\n",
    "def getPartition(timestamp_ms):\n",
    "    return int(timestamp_ms / (3600 * 1000)) # divide milliseconds to get hours\n",
    "\n",
    "def dropConstraints(graph):\n",
    "    \n",
    "    constraints = [\n",
    "        \"DROP CONSTRAINT ON (t:Tweet) ASSERT t.id_str IS UNIQUE;\",\n",
    "        \"DROP CONSTRAINT ON (u:User) ASSERT u.screen_name IS UNIQUE;\",\n",
    "        \"DROP CONSTRAINT ON (h:Hashtag) ASSERT h.name IS UNIQUE;\",\n",
    "        \"DROP CONSTRAINT ON (l:Link) ASSERT l.url IS UNIQUE;\",\n",
    "        \"DROP CONSTRAINT ON (s:Source) ASSERT s.source IS UNIQUE;\",\n",
    "        \"DROP CONSTRAINT ON (p:Place) ASSERT p.id IS UNIQUE;\",\n",
    "        \"DROP CONSTRAINT ON (t:Tweet) ASSERT t.id IS UNIQUE;\",\n",
    "        \"DROP CONSTRAINT ON (t:Tweet) ASSERT t.partition_key IS UNIQUE;\",\n",
    "        \"DROP CONSTRAINT ON (u:User) ASSERT u.partition_key IS UNIQUE;\",\n",
    "        \"DROP CONSTRAINT ON (h:Hashtag) ASSERT h.partition_key IS UNIQUE;\",\n",
    "        \"DROP CONSTRAINT ON (l:Link) ASSERT l.partition_key IS UNIQUE;\",\n",
    "        \"DROP CONSTRAINT ON (s:Source) ASSERT s.partition_key IS UNIQUE;\",\n",
    "        \"DROP CONSTRAINT ON (p:Place) ASSERT p.partition_key IS UNIQUE;\"\n",
    "    ]\n",
    "    for constraint in constraints:\n",
    "        try:\n",
    "            graph.cypher.execute(constraint)\n",
    "            print constraint\n",
    "        except Exception as e:\n",
    "            print 'Could not drop the following constraint -- ', constraint\n",
    "\n",
    "def setConstraints(graph, partition_key):\n",
    "    constraints = [\n",
    "        \"\"\"CREATE CONSTRAINT ON (t:Tweet_{0}) ASSERT t.id_str IS UNIQUE;\"\"\".format(partition_key),\n",
    "        \"\"\"CREATE CONSTRAINT ON (u:User_{0}) ASSERT u.screen_name IS UNIQUE;\"\"\".format(partition_key),\n",
    "        \"\"\"CREATE CONSTRAINT ON (h:Hashtag_{0}) ASSERT h.name IS UNIQUE;\"\"\".format(partition_key),\n",
    "        \"\"\"CREATE CONSTRAINT ON (l:Link_{0}) ASSERT l.url IS UNIQUE;\"\"\".format(partition_key),\n",
    "        \"\"\"CREATE CONSTRAINT ON (s:Source_{0}) ASSERT s.source IS UNIQUE;\"\"\".format(partition_key),\n",
    "        \"\"\"CREATE CONSTRAINT ON (p:Place_{0}) ASSERT p.id IS UNIQUE;\"\"\".format(partition_key),\n",
    "        \"\"\"CREATE INDEX ON :Tweet_{0}(timestamp_int);\"\"\".format(partition_key)\n",
    "    ]\n",
    "    for constraint in constraints:\n",
    "        try:\n",
    "            graph.cypher.execute(constraint)\n",
    "            print constraint\n",
    "        except Exception as e:\n",
    "            print 'Could not create the following constraint -- ', constraint\n",
    "\n",
    "\n",
    "def tweetQuery(partition_key):\n",
    "    \n",
    "    tweetstr = \"\"\"MERGE (tweet:Tweet_{0}:Tweet {{id_str:t.id_str}})\n",
    "        ON CREATE SET tweet.text = t.text,\n",
    "            tweet.created_at = t.created_at,\n",
    "            tweet.favorite_count = t.favorite_count,\n",
    "            tweet.timestamp_int = toInt(t.timestamp_ms)\n",
    "        ON MATCH SET tweet += {{\n",
    "            favorite_count:t.favorite_count\n",
    "        }}\n",
    "        FOREACH (coord IN [c IN [t.coordinates] WHERE c IS NOT NULL] |\n",
    "            SET tweet.coordinates = coord.coordinates\n",
    "        )\"\"\".format(partition_key)\n",
    "#     print 'tweet', tweetstr\n",
    "    return tweetstr\n",
    "\n",
    "def userQuery(partition_key):\n",
    "    \n",
    "    userstr = \"\"\"\n",
    "        MERGE (user:User_{0}:User {{screen_name:u.screen_name}})\n",
    "        ON CREATE SET user.name = u.name,\n",
    "            user.location = u.location,\n",
    "            user.followers = u.followers_count,\n",
    "            user.description = u.description,\n",
    "            user.following = u.friends_count,\n",
    "            user.time_zone = u.time_zone,\n",
    "            user.statuses_count = u.statuses_count,\n",
    "            user.verified = u.verified\n",
    "        ON MATCH SET user += {{\n",
    "            location : u.location,\n",
    "            followers : u.followers_count,\n",
    "            description : u.description,\n",
    "            following : u.friends_count,\n",
    "            time_zone : u.time_zone,\n",
    "            statuses_count : u.statuses_count,\n",
    "            verified : u.verified\n",
    "        }}\n",
    "        MERGE (user)-[:POSTS]->(tweet)\"\"\".format(partition_key)\n",
    "#     print 'user', userstr\n",
    "    return userstr\n",
    "\n",
    "def placeQuery(partition_key):\n",
    "    \n",
    "    placestr = \"\"\"\n",
    "        FOREACH (pid IN p.id |\n",
    "            MERGE (place:Place_{0}:Place {{id:p.id}})\n",
    "            ON CREATE SET place.url=p.url,\n",
    "                place.id_str=p.id,\n",
    "                place.place_type=p.place_type,\n",
    "                place.country=p.country,\n",
    "                place.name=p.name,\n",
    "                place.full_name=p.full_name,\n",
    "                place.country_code=p.country_code,\n",
    "                place.country=p.country\n",
    "            MERGE (tweet)-[:LOCATED_IN]->(place)\n",
    "        )\"\"\".format(partition_key)\n",
    "#     print 'place', placestr\n",
    "    return placestr\n",
    "\n",
    "def sourceQuery(partition_key):\n",
    "    \n",
    "    sourcestr = \"\"\"\n",
    "        MERGE (source:Source_{0}:Source {{source:t.source}})\n",
    "        MERGE (tweet)-[:USING]->(source)\"\"\".format(partition_key)\n",
    "#     print 'source', sourcestr\n",
    "    return sourcestr\n",
    "\n",
    "def entitiesQuery(partition_key):\n",
    "    \n",
    "    entitiesstr = \"\"\"\n",
    "        FOREACH (h IN e.hashtags |\n",
    "            MERGE (tag:Hashtag_{0}:Hashtag {{name:LOWER(h.text)}})\n",
    "            MERGE (tag)-[:TAGS{{indices:h.indices}}]->(tweet)\n",
    "        )\n",
    "        FOREACH (u IN e.urls |\n",
    "            MERGE (url:Link_{0}:Link {{url:u.expanded_url}})\n",
    "            MERGE (tweet)-[:CONTAINS{{indices:u.indices}}]->(url)\n",
    "        )\n",
    "        FOREACH (m IN e.user_mentions |\n",
    "            MERGE (mentioned:User_{0}:User {{screen_name:m.screen_name}})\n",
    "            ON CREATE SET mentioned.name = m.name\n",
    "            MERGE (tweet)-[:MENTIONS{{indices:m.indices}}]->(mentioned)\n",
    "        )\"\"\".format(partition_key)\n",
    "#     print 'entitities', entitiesstr\n",
    "    return entitiesstr\n",
    "\n",
    "def replytoQuery(partition_key):\n",
    "    \n",
    "    replytostr = \"\"\"\n",
    "        FOREACH (r IN [r IN [t.in_reply_to_status_id_str] WHERE r IS NOT NULL] |\n",
    "            MERGE (reply_tweet:Tweet_{0}:Tweet {{id_str:r}})\n",
    "            MERGE (tweet)-[:REPLY_TO]->(reply_tweet)\n",
    "        )\"\"\".format(partition_key)\n",
    "#     print 'replyto', replytostr\n",
    "    return replytostr\n",
    "\n",
    "def retweetQuery(partition_key):\n",
    "    \n",
    "    retweetstr = \"\"\"\n",
    "    FOREACH (retweet_id IN [x IN [retweet.id_str] WHERE x IS NOT NULL] |\n",
    "            MERGE (retweet_tweet:Tweet_{0}:Tweet {{id_str:retweet_id}})\n",
    "            MERGE (tweet)-[:RETWEETS]->(retweet_tweet)\n",
    "        )\"\"\".format(partition_key)\n",
    "#     print 'retweet', retweetstr\n",
    "    return retweetstr\n",
    "        \n",
    "def runQuery(graph,tweets, partition_key):\n",
    "    \n",
    "# Pass dict to Cypher and build query.\n",
    "    query = \"\"\"\n",
    "        WITH {json} as data\n",
    "        UNWIND data.tweets as t \n",
    "        WITH t\n",
    "        ORDER BY t.id\n",
    "        WITH t,\n",
    "             t.entities AS e,\n",
    "             t.user AS u,\n",
    "             t.place AS p,\n",
    "             t.retweeted_status AS retweet\n",
    "    \"\"\"\\\n",
    "    + tweetQuery(partition_key)\\\n",
    "    + userQuery(partition_key)\\\n",
    "    + placeQuery(partition_key)\\\n",
    "    + sourceQuery(partition_key)\\\n",
    "    + entitiesQuery(partition_key)\\\n",
    "    + replytoQuery(partition_key)\\\n",
    "    + retweetQuery(partition_key)\n",
    "    \n",
    "    #print query\n",
    "    \n",
    "    try:\n",
    "        params = dict(json=tweets)\n",
    "        graph.cypher.execute(query, params)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "def writebatch(graph, linescount, start, databatch,partition_key,partitionsize):\n",
    "    try:\n",
    "\n",
    "        if (partitionsize > 0):\n",
    "            partition_key = partition_key + '_' + str(linescount/partitionsize)\n",
    "        \n",
    "        runQuery(graph,databatch,partition_key)\n",
    "                            \n",
    "        databatch['tweets'] = []\n",
    "                            \n",
    "        print \"{0} lines processed after {1} seconds.\".format(linescount,time.time()-start)\n",
    "   \n",
    "    except:\n",
    "        print \"error at record {0}\".format(linescount)\n",
    "    \n",
    "def main(argv):\n",
    "    \n",
    "    fname, password, remoteserver, readsize, batchsize, partition_key, constraints, partitionsize, startval = readargs(argv)\n",
    "    print partition_key\n",
    "\n",
    "    graph = authenticate(password, remoteserver)\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "# Connect to graph and add constraints.\n",
    "\n",
    "    if (constraints):\n",
    "        #dropConstraints(graph)\n",
    "        setConstraints(graph, partition_key)\n",
    "        \n",
    "    databatch = {'tweets':[]}\n",
    "\n",
    "    with open(fname, 'rb') as f:\n",
    "        i = 0;\n",
    "        j = 0;\n",
    "  \n",
    "        for line in f:  \n",
    "                \n",
    "            if ((line is None) or (j >= readsize)): #test end of file, write remaining data\n",
    "                if (i>0):\n",
    "                    writebatch (graph, j, start, databatch,partition_key,partitionsize)\n",
    "                break\n",
    "\n",
    "            j=j+1\n",
    "            \n",
    "            if (j >= startval):\n",
    "                \n",
    "                i=i+1\n",
    "\n",
    "                try:\n",
    "\n",
    "                    data = json.loads(line, encoding='utf8')\n",
    "\n",
    "                    if ('limit' not in data.keys()):\n",
    "                        addtweet(data,databatch)\n",
    "\n",
    "                    if ((i == batchsize) or (i==readsize)): \n",
    "                        writebatch (graph, j, start, databatch,partition_key,partitionsize)\n",
    "                        i = 0    \n",
    "\n",
    "                except Exception as e:\n",
    "                    print 'final error encountered...', e, j, \"lines processed\"\n",
    "    \n",
    "        end = time.time()\n",
    "        print end - start\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv[1:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !python json_twitter_loader.py -i 'data/election172' -r 'http://localhost:7474/db/data' \\\n",
    "# -m 20000000 -b 1000 -k '172' -c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n",
      "717999 lines processed after 22.6590359211 seconds.\n",
      "718999 lines processed after 50.6272208691 seconds.\n",
      "719999 lines processed after 73.9657549858 seconds.\n",
      "720999 lines processed after 94.8544158936 seconds.\n",
      "721999 lines processed after 115.225142956 seconds.\n",
      "722999 lines processed after 141.238190889 seconds.\n",
      "723999 lines processed after 165.504703999 seconds.\n",
      "724999 lines processed after 186.799711943 seconds.\n",
      "725999 lines processed after 210.359490871 seconds.\n",
      "726999 lines processed after 231.040259838 seconds.\n",
      "727999 lines processed after 255.038166046 seconds.\n",
      "728999 lines processed after 275.277183056 seconds.\n",
      "729999 lines processed after 294.837776899 seconds.\n",
      "730999 lines processed after 312.347166061 seconds.\n",
      "731999 lines processed after 332.313933849 seconds.\n",
      "732999 lines processed after 351.764287949 seconds.\n",
      "733999 lines processed after 376.340904951 seconds.\n",
      "734999 lines processed after 397.749153852 seconds.\n",
      "735999 lines processed after 420.891018867 seconds.\n",
      "736999 lines processed after 443.247554064 seconds.\n",
      "737999 lines processed after 472.418756008 seconds.\n",
      "738999 lines processed after 494.924870968 seconds.\n",
      "739999 lines processed after 516.641855001 seconds.\n",
      "740999 lines processed after 539.777760029 seconds.\n",
      "741999 lines processed after 566.503802061 seconds.\n",
      "742999 lines processed after 588.834262848 seconds.\n",
      "743999 lines processed after 611.663619995 seconds.\n",
      "744999 lines processed after 633.374492884 seconds.\n",
      "745999 lines processed after 658.259953976 seconds.\n",
      "746999 lines processed after 682.938079834 seconds.\n",
      "747999 lines processed after 703.753914833 seconds.\n",
      "748999 lines processed after 726.295944929 seconds.\n",
      "749999 lines processed after 752.141939878 seconds.\n",
      "750999 lines processed after 774.352915049 seconds.\n",
      "751999 lines processed after 797.42185092 seconds.\n",
      "752999 lines processed after 820.770697832 seconds.\n",
      "753999 lines processed after 845.659981966 seconds.\n",
      "754999 lines processed after 866.947180986 seconds.\n",
      "755999 lines processed after 888.660326958 seconds.\n",
      "756999 lines processed after 912.828075886 seconds.\n",
      "757999 lines processed after 937.607880831 seconds.\n",
      "758999 lines processed after 959.729526997 seconds.\n",
      "759999 lines processed after 978.363167048 seconds.\n",
      "760999 lines processed after 1005.80443096 seconds.\n",
      "761999 lines processed after 1029.05264997 seconds.\n",
      "762999 lines processed after 1050.6534729 seconds.\n",
      "763999 lines processed after 1074.92902589 seconds.\n",
      "764999 lines processed after 1099.40135384 seconds.\n",
      "765999 lines processed after 1124.90671802 seconds.\n",
      "766999 lines processed after 1148.14014506 seconds.\n",
      "767999 lines processed after 1172.51372385 seconds.\n",
      "768999 lines processed after 1194.94147587 seconds.\n",
      "769999 lines processed after 1220.13260198 seconds.\n",
      "770999 lines processed after 1237.94990993 seconds.\n",
      "771999 lines processed after 1259.76034188 seconds.\n",
      "772999 lines processed after 1284.64249802 seconds.\n",
      "773999 lines processed after 1306.58871484 seconds.\n",
      "774999 lines processed after 1328.88171792 seconds.\n",
      "775999 lines processed after 1352.84271502 seconds.\n",
      "776999 lines processed after 1376.043432 seconds.\n",
      "777999 lines processed after 1390.33292389 seconds.\n",
      "778999 lines processed after 1414.28180885 seconds.\n",
      "779999 lines processed after 1433.42628288 seconds.\n",
      "780999 lines processed after 1453.60198092 seconds.\n",
      "781999 lines processed after 1480.7346139 seconds.\n",
      "782999 lines processed after 1502.14262295 seconds.\n",
      "783999 lines processed after 1524.56529999 seconds.\n",
      "784999 lines processed after 1540.23369503 seconds.\n",
      "785999 lines processed after 1559.07171106 seconds.\n",
      "786999 lines processed after 1580.47847486 seconds.\n",
      "787999 lines processed after 1597.48966384 seconds.\n",
      "788999 lines processed after 1624.2026 seconds.\n",
      "789999 lines processed after 1642.32499194 seconds.\n",
      "790999 lines processed after 1662.29677606 seconds.\n",
      "791999 lines processed after 1682.87665296 seconds.\n",
      "792999 lines processed after 1702.1347239 seconds.\n",
      "793999 lines processed after 1724.35187197 seconds.\n",
      "794999 lines processed after 1744.62572885 seconds.\n",
      "795999 lines processed after 1759.68995094 seconds.\n",
      "796999 lines processed after 1780.88867188 seconds.\n",
      "797999 lines processed after 1799.61394501 seconds.\n",
      "798999 lines processed after 1825.62446284 seconds.\n",
      "799999 lines processed after 1847.8436439 seconds.\n",
      "800999 lines processed after 1871.72567987 seconds.\n",
      "801999 lines processed after 1898.93842793 seconds.\n",
      "802999 lines processed after 1918.81119084 seconds.\n",
      "803999 lines processed after 1940.73544192 seconds.\n",
      "804999 lines processed after 1966.32865596 seconds.\n",
      "805999 lines processed after 1992.13161993 seconds.\n",
      "806999 lines processed after 2013.33104992 seconds.\n",
      "807999 lines processed after 2035.75683904 seconds.\n",
      "808999 lines processed after 2056.95076299 seconds.\n",
      "809999 lines processed after 2076.09614491 seconds.\n",
      "810999 lines processed after 2102.82358003 seconds.\n",
      "811999 lines processed after 2122.48403883 seconds.\n",
      "812999 lines processed after 2136.82122183 seconds.\n",
      "813999 lines processed after 2151.358284 seconds.\n",
      "814999 lines processed after 2165.69509387 seconds.\n",
      "815999 lines processed after 2181.98826289 seconds.\n",
      "816999 lines processed after 2201.64246297 seconds.\n",
      "817999 lines processed after 2225.30739999 seconds.\n",
      "818999 lines processed after 2246.59357595 seconds.\n",
      "819999 lines processed after 2266.46551204 seconds.\n",
      "820999 lines processed after 2286.53044701 seconds.\n",
      "821999 lines processed after 2310.79972005 seconds.\n",
      "822999 lines processed after 2334.35691094 seconds.\n",
      "823999 lines processed after 2354.01326203 seconds.\n",
      "824999 lines processed after 2378.07558584 seconds.\n",
      "825999 lines processed after 2398.25506997 seconds.\n",
      "826999 lines processed after 2428.36437893 seconds.\n",
      "827999 lines processed after 2458.56607389 seconds.\n",
      "828999 lines processed after 2480.89016795 seconds.\n",
      "829999 lines processed after 2500.95652699 seconds.\n",
      "830999 lines processed after 2523.99452806 seconds.\n",
      "831999 lines processed after 2550.622756 seconds.\n",
      "832999 lines processed after 2575.50932503 seconds.\n",
      "833999 lines processed after 2599.47176695 seconds.\n",
      "834999 lines processed after 2624.85673189 seconds.\n",
      "835999 lines processed after 2647.80026102 seconds.\n",
      "836999 lines processed after 2673.50661802 seconds.\n",
      "837999 lines processed after 2696.24068499 seconds.\n",
      "838999 lines processed after 2721.43013692 seconds.\n",
      "839999 lines processed after 2747.23382688 seconds.\n",
      "840999 lines processed after 2770.58700204 seconds.\n",
      "841999 lines processed after 2795.78329897 seconds.\n",
      "842999 lines processed after 2817.16744804 seconds.\n",
      "2817.41185999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in ['191']:\n",
    "    \n",
    "    filename = './archive/data/election{0}'.format(i)\n",
    "    partitionkey = '{0}'.format(i)\n",
    "    \n",
    "    !python cypher_data_loader.py -i $filename \\\n",
    "    -r 'http://localhost:7474/db/data' \\\n",
    "    -s 717000 -m 1000000 -b 1000 -k $partitionkey\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
