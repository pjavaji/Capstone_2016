{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# May 2016\n",
    "# Debbie Hofman\n",
    "# Pre-process node and edges for loading with neo4j batch loader\n",
    "# Use UTC day partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data_preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_preprocess.py\n",
    "\n",
    "import sys, getopt\n",
    "import json\n",
    "import datetime\n",
    "import pytz\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "from tweetparser_timepart import tweetparser\n",
    "\n",
    "epoch = datetime.datetime.utcfromtimestamp(0)\n",
    "\n",
    "nodetables = ['Tweet','User','Source','Place','Hashtag','Link','UserMention']\n",
    "reltables = ['POSTS','USING','LOCATED_IN','TAGS','CONTAINS','MENTIONS','RETWEETS','REPLY_TO']\n",
    "\n",
    "\n",
    "def readargs(argv):\n",
    "    \n",
    "    inputfile=''\n",
    "    remoteserver=''\n",
    "    maxsize=2000\n",
    "    batchsize=1000\n",
    "    truncate=False\n",
    "    \n",
    "    try:\n",
    "        opts, args = getopt.getopt(argv,\"hi:p:r:m:b:t\")\n",
    "    except getopt.GetoptError:\n",
    "        print 'data_preprocess.py -i <inputfile> -r <remote server url>'\n",
    "        sys.exit(2)\n",
    "    for opt, arg in opts:\n",
    "        if opt == '-h':\n",
    "            print 'data_preprocess.py -i <inputfile> -r <remote server url> \\\n",
    "            -m <maxnumberrows> -b <batchsize> -t<truncate>'\n",
    "            sys.exit()\n",
    "        elif opt == \"-i\":\n",
    "            inputfile = arg\n",
    "        elif opt == \"-b\":\n",
    "            batchsize = int(arg)\n",
    "        elif opt == \"-m\":\n",
    "            maxsize = int(arg)\n",
    "        elif opt == \"-t\":\n",
    "            truncate = True\n",
    "        \n",
    "    return inputfile, batchsize, maxsize, truncate\n",
    "\n",
    "   \n",
    "def unix_time_millis(dt):\n",
    "    return int( (dt - epoch).total_seconds() * 1000.0)\n",
    "\n",
    "\n",
    "def addtweet(data,databatch):\n",
    "    \n",
    "    pretweet=''\n",
    "    \n",
    "    if (('retweeted' in data) and ('retweeted_status' in data)):\n",
    "        pretweet = data['retweeted_status'];\n",
    "\n",
    "    elif (('quoted_status' in data)):\n",
    "        pretweet = data['quoted_status'];\n",
    "        \n",
    "    if (pretweet != ''):\n",
    "        databatch.append(pretweet) \n",
    "    \n",
    "    databatch.append(data)\n",
    "    \n",
    "    \n",
    "def get_idcols(node_or_rel, columns):\n",
    "    \n",
    "    if (node_or_rel=='node'):\n",
    "        return [col for col in columns if ':ID' in col]\n",
    "\n",
    "    else:  \n",
    "        startcol = [col for col in columns if ':START_ID' in col]\n",
    "        endcol = [col for col in columns if ':END_ID' in col]\n",
    "        typecol = [col for col in columns if ':TYPE' in col]\n",
    "\n",
    "        return startcol+endcol+typecol\n",
    "    \n",
    "\n",
    "def get_filename(filenamepart, partition_key):\n",
    "    \n",
    "    filename='dataloader6/output'+'/'+partition_key+'/'+filenamepart+'.csv'\n",
    "    \n",
    "    return filename\n",
    "\n",
    "\n",
    "def checkpath(partition_key):\n",
    "    path = 'dataloader6/output'+'/'+ partition_key\n",
    "    try: \n",
    "        os.makedirs(path)\n",
    "    except OSError:\n",
    "        if not os.path.isdir(path):\n",
    "            raise\n",
    "\n",
    "\n",
    "def getheader(filenamepart, partition_key):\n",
    "    \n",
    "    headernamepart=filenamepart+\"-header\"\n",
    "    filename=get_filename(headernamepart, partition_key)\n",
    "\n",
    "    columns=[]\n",
    "    with open(filename, 'r+') as f:\n",
    "        for line in f:\n",
    "            columns = line.split(',')\n",
    "    \n",
    "    #print columns\n",
    "    return columns\n",
    "\n",
    "\n",
    "def writeheader(filenamepart, partition_key, columns):\n",
    "    #print filenamepart, columns\n",
    "    headernamepart=filenamepart+\"-header\"\n",
    "    filename=get_filename(headernamepart, partition_key)\n",
    "    \n",
    "    if not os.path.isfile(filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(','.join(columns))\n",
    "\n",
    "            \n",
    "def writefile(rowlist, filenamepart, node_or_rel, partition_key):\n",
    "\n",
    "    if (len(rowlist) > 0):\n",
    "        \n",
    "        #create a dataframe and drop duplicates\n",
    "        columns=rowlist[0].keys()\n",
    "        \n",
    "        df = pd.DataFrame(rowlist, columns=columns)\n",
    "        \n",
    "        df = df.drop_duplicates(subset=get_idcols(node_or_rel, rowlist[0].keys()))\n",
    "        df = df[columns]\n",
    "    \n",
    "        #make sure path exists and write\n",
    "        checkpath(partition_key)\n",
    "        \n",
    "        #make sure header file exists..\n",
    "        writeheader(filenamepart, partition_key, columns)\n",
    "        \n",
    "        #get filename\n",
    "        filename=get_filename(filenamepart, partition_key)\n",
    "        \n",
    "        df.to_csv(filename,\\\n",
    "            index=False,\\\n",
    "            header=False,\\\n",
    "            encoding=\"utf-8\",\\\n",
    "            mode=\"a\")\n",
    "        \n",
    "        \n",
    "def init_file(filenamepart, partition_key) :\n",
    "    \n",
    "    filename=get_filename(filenamepart, partition_key)\n",
    "    \n",
    "    if os.path.isfile(filename):\n",
    "        os.remove(filename)\n",
    "\n",
    "        \n",
    "def dedup_file(filenamepart, node_or_rel, partition_key) :\n",
    "\n",
    "    filename=get_filename(filenamepart, partition_key)\n",
    "    \n",
    "    if os.path.isfile(filename):\n",
    "        \n",
    "        try:\n",
    "            #Now, read back in the data and dedup one more time (previously dedup within each small batch)\n",
    "            df=pd.read_csv(filename, encoding=\"utf-8\", dtype={'retweet_id':'str'}, error_bad_lines=False)\n",
    "\n",
    "            columns = getheader(filenamepart, partition_key)\n",
    "            #print columns\n",
    "            df.columns = columns\n",
    "\n",
    "            df = df.drop_duplicates(subset=get_idcols(node_or_rel, df.columns))\n",
    "            \n",
    "            outfile = get_filename(filenamepart+'_dedup', partition_key)\n",
    "            init_file(filenamepart+'_dedup', partition_key)\n",
    "\n",
    "            df.to_csv(outfile, \\\n",
    "                      header=False,\\\n",
    "                      index=False,\\\n",
    "                      encoding=\"utf-8\", \\\n",
    "                      mode=\"w+\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print \"error while deduping files\"\n",
    "            print e\n",
    "            \n",
    "\n",
    "def initfiles(partition_key):\n",
    "    \n",
    "    try:\n",
    "                \n",
    "        for k in nodetables:\n",
    "            init_file(k, partition_key)\n",
    "                            \n",
    "        for k in reltables:\n",
    "            init_file(k, partition_key)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print \"error while initializing files\"\n",
    "        print e\n",
    "\n",
    "        \n",
    "def writebatch(start, linescount, databatch, parser, partition_key):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        parser.parseData(databatch, partition_key)\n",
    "        \n",
    "        for k,v in parser.nodelists.iteritems():\n",
    "            writefile(v, k, 'node', partition_key)\n",
    "                            \n",
    "        for k,v in parser.rellists.iteritems():\n",
    "            writefile(v, k, 'rel', partition_key)\n",
    "        \n",
    "        parser.clear()\n",
    "        \n",
    "        del databatch[:]\n",
    "            \n",
    "        print \"{0} lines processed after {1} seconds.\".format(linescount,time.time()-start)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print \"error at record {0}\".format(linescount)\n",
    "        print e\n",
    "\n",
    "\n",
    "def dedup_files(start, partition_key):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        for k in nodetables:\n",
    "            dedup_file(k, 'node', partition_key)\n",
    "            print \"dedup {0} complete after {1} seconds.\".format(k,time.time()-start)\n",
    "                            \n",
    "        for k in reltables:\n",
    "            dedup_file(k, 'rel', partition_key)\n",
    "            print \"dedup {0} complete after {1} seconds.\".format(k,time.time()-start)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print \"error while deduping files\"\n",
    "        print e\n",
    "        raise\n",
    "    \n",
    "    \n",
    "def fix_edges(partition_key, edge_file, node_file):\n",
    "    \n",
    "    try:\n",
    "\n",
    "        #Get edge file, column list, id columns\n",
    "        df1filename = get_filename(edge_file+'_dedup', partition_key)\n",
    "        df2filename = get_filename(node_file+'_dedup', partition_key)\n",
    "        \n",
    "        if ((os.path.isfile(df1filename)) and (os.path.isfile(df2filename))):\n",
    "            \n",
    "            df1=pd.read_csv(df1filename)\n",
    "            df1.columns = getheader(edge_file, partition_key)\n",
    "            #df1columns = df1.columns\n",
    "            edgeidcols = get_idcols('rel', df1.columns)\n",
    "\n",
    "            #Get node file, id columns\n",
    "            df2=pd.read_csv(df2filename)\n",
    "            df2.columns = getheader(node_file, partition_key)\n",
    "            nodeidcols = get_idcols('node', df2.columns)\n",
    "\n",
    "            #merge tables (left join on edge file)\n",
    "            key1 = edgeidcols[1]\n",
    "            key2 = nodeidcols[0]\n",
    "\n",
    "            df3=pd.merge(df1, df2, left_on=key1, right_on=key2, how = 'left')\n",
    "\n",
    "            df3 = df3[(df3[key2] == df3[key1])]\n",
    "            df3 = df3[df1.columns]\n",
    "\n",
    "            #remove any lines that have null values. Edges should never have null values.\n",
    "            df3.dropna(inplace=True)\n",
    "\n",
    "            #write out modified edge file with bad references removed.\n",
    "            df3.to_csv(df1filename,\\\n",
    "                header=False,\\\n",
    "                index=False,\\\n",
    "                encoding=\"utf-8\",\\\n",
    "                mode=\"w+\")\n",
    "    \n",
    "    except Exception as e:\n",
    "            print \"error fixes edges\"\n",
    "            print e\n",
    "            raise\n",
    "\n",
    "def finalizepartition(start, partitionkey):\n",
    "    \n",
    "    print 'dedup partition', partitionkey\n",
    "    dedup_files(start, partitionkey)\n",
    "                        \n",
    "    print 'fix edges partition', partitionkey\n",
    "    fix_edges(partitionkey, 'REPLY_TO', 'Tweet')\n",
    "    fix_edges(partitionkey, 'MENTIONS', 'User')\n",
    "    \n",
    "    print 'completed partition ', partitionkey\n",
    "\n",
    "    \n",
    "def is_en(data):\n",
    "    \n",
    "    lang=''\n",
    "    \n",
    "    if (('lang' in data) and (data['lang']!=None)):\n",
    "        lang=data['lang']\n",
    "        \n",
    "    return (lang=='en') #return boolean\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    \n",
    "    fname, batchsize, readsize, truncate = readargs(argv)\n",
    "    print \"file=\", fname, \"key=\", \"batch=\", batchsize, \"max=\", readsize, \"truncate=\", truncate\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    parser = tweetparser()\n",
    "# Connect to graph and add constraints.\n",
    "        \n",
    "    databatch = []\n",
    "    partitions = []\n",
    "    limitlines = 0\n",
    "\n",
    "    with open(fname, 'rb') as f:\n",
    "        i = 0;\n",
    "        j = 0;\n",
    "        lastpartitionkey=''\n",
    "  \n",
    "        for line in f:  \n",
    "            \n",
    "            if ((line is None) or (j >= readsize)): #test end of file, write remaining data\n",
    "                break;\n",
    "            \n",
    "            j=j+1 \n",
    "            i=i+1\n",
    "        \n",
    "            try:\n",
    "            \n",
    "                data = json.loads(line, encoding='utf8')\n",
    "                \n",
    "                if (('limit' not in data.keys()) and (is_en(data))):\n",
    "\n",
    "                    #update partition key based on content of this row\n",
    "                    partition_key = parser.getPartitionKey(data)\n",
    "                    \n",
    "                    newpartition = ((lastpartitionkey!='') and (partition_key != lastpartitionkey))\n",
    "                    \n",
    "                    if (newpartition):\n",
    "                        \n",
    "                        print 'new partition found',lastpartitionkey,partition_key,j\n",
    "                        \n",
    "                        writebatch(start, j, databatch, parser, lastpartitionkey)\n",
    "                        i=0\n",
    "                        \n",
    "                        print 'reset partition key'\n",
    "                        partitions.append(partition_key)\n",
    "                        lastpartitionkey = partition_key\n",
    "                        print lastpartitionkey\n",
    "                          \n",
    "                    addtweet(data,databatch)\n",
    "                                     \n",
    "                    if (lastpartitionkey==''):\n",
    "                        lastpartitionkey = partition_key\n",
    "                        partitions.append(partition_key)\n",
    "\n",
    "                else:\n",
    "                    limitlines = limitlines + 1\n",
    "                    \n",
    "                                        \n",
    "                if ((i == batchsize) or (i==readsize)): \n",
    "                    writebatch (start, j, databatch, parser, partition_key)\n",
    "                    i = 0 \n",
    "                    \n",
    "            except Exception as e:\n",
    "                #print data, databatch\n",
    "                print 'error encountered...', e, j, \"lines processed\"\n",
    "    \n",
    "        if (len(databatch)>0):\n",
    "            writebatch (start, j, databatch, parser, lastpartitionkey)\n",
    "            \n",
    "        partitions = list(set(partitions))\n",
    "        \n",
    "        for partitionkey in partitions:\n",
    "            finalizepartition(start,partitionkey)\n",
    "        \n",
    "        end = time.time()\n",
    "        print end - start\n",
    "        \n",
    "        print 'limitlines', limitlines\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv[1:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file= ./archive/data/election172 key= batch= 10000 max= 1000000 truncate= False\n",
      "10000 lines processed after 5.71085500717 seconds.\n",
      "20000 lines processed after 11.7520539761 seconds.\n",
      "30000 lines processed after 17.2265031338 seconds.\n",
      "^CTraceback (most recent call last):\n",
      "  File \"data_preprocess.py\", line 395, in <module>\n",
      "    main(sys.argv[1:])\n",
      "  File \"data_preprocess.py\", line 373, in main\n",
      "    writebatch (start, j, databatch, parser, partition_key)\n",
      "  File \"data_preprocess.py\", line 215, in writebatch\n",
      "    writefile(v, k, 'rel', partition_key)\n",
      "  File \"data_preprocess.py\", line 149, in writefile\n",
      "    mode=\"a\")\n",
      "  File \"/Users/debbiehofman/anaconda/lib/python2.7/site-packages/pandas/core/frame.py\", line 1189, in to_csv\n",
      "    formatter.save()\n",
      "  File \"/Users/debbiehofman/anaconda/lib/python2.7/site-packages/pandas/core/format.py\", line 1467, in save\n",
      "    self._save()\n",
      "  File \"/Users/debbiehofman/anaconda/lib/python2.7/site-packages/pandas/core/format.py\", line 1567, in _save\n",
      "    self._save_chunk(start_i, end_i)\n",
      "  File \"/Users/debbiehofman/anaconda/lib/python2.7/site-packages/pandas/core/format.py\", line 1594, in _save_chunk\n",
      "    lib.write_csv_rows(self.data, ix, self.nlevels, self.cols, self.writer)\n",
      "  File \"pandas/lib.pyx\", line 992, in pandas.lib.write_csv_rows (pandas/lib.c:17912)\n",
      "  File \"/Users/debbiehofman/anaconda/lib/python2.7/site-packages/pandas/core/common.py\", line 2918, in writerows\n",
      "    self.stream.write(data)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python data_preprocess.py -i './archive/data/election172' -m 1000000 -b 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # # for i in range(172,190):\n",
    "# %cd ~/documents/DSE/dhofman/capstone\n",
    "# import os\n",
    "# files = os.listdir(\"./archive/data\")\n",
    "# print files\n",
    "# for fn in files: \n",
    "#     filename = \"./archive/data/\"+fn\n",
    "#     !python data_preprocess.py -i $filename -m 1200000 -b 10000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test code below for data cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#!head -895565 'dataloader2/output/2016_3_2/UserMention.csv' |tail -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "        \n",
    "def init_file(filenamepart, partition_key) :\n",
    "    \n",
    "    filename=get_filename(filenamepart, partition_key)\n",
    "    \n",
    "    if os.path.isfile(filename):\n",
    "        os.remove(filename)\n",
    "\n",
    "        \n",
    "def get_idcols(node_or_rel, columns):\n",
    "    \n",
    "    if (node_or_rel=='node'):\n",
    "        return [col for col in columns if ':ID' in col]\n",
    "\n",
    "    else:  \n",
    "        startcol = [col for col in columns if ':START_ID' in col]\n",
    "        endcol = [col for col in columns if ':END_ID' in col]\n",
    "        typecol = [col for col in columns if ':TYPE' in col]\n",
    "\n",
    "        return startcol+endcol+typecol\n",
    "    \n",
    "\n",
    "def getheader(filenamepart, partition_key):\n",
    "    \n",
    "    headernamepart=filenamepart+\"-header\"\n",
    "    filename=get_filename(headernamepart, partition_key)\n",
    "\n",
    "    columns=[]\n",
    "    with open(filename, 'r+') as f:\n",
    "        for line in f:\n",
    "            columns = line.split(',')\n",
    "    \n",
    "    #print columns\n",
    "    return columns\n",
    "\n",
    "def get_filename(filenamepart, partition_key):\n",
    "    \n",
    "    filename='dataloader4/output'+'/'+partition_key+'/'+filenamepart+'.csv'\n",
    "    \n",
    "    return filename\n",
    "\n",
    "\n",
    "def dedup_file_test(filenamepart, node_or_rel, partition_key) :\n",
    "\n",
    "    filename=get_filename(filenamepart, partition_key)\n",
    "    \n",
    "    if os.path.isfile(filename):\n",
    "        \n",
    "        try:\n",
    "            #Now, read back in the data and dedup one more time (previously dedup within each small batch)\n",
    "            df=pd.read_csv(filename, encoding=\"utf-8\", dtype={'retweet_id':'str'}, error_bad_lines=False)\n",
    "\n",
    "            columns = getheader(filenamepart, partition_key)\n",
    "            #print columns\n",
    "            df.columns = columns\n",
    "\n",
    "            df = df.drop_duplicates(subset=get_idcols(node_or_rel, df.columns))\n",
    "            \n",
    "            outfile = get_filename(filenamepart+'_dedup', partition_key)\n",
    "            init_file(filenamepart+'_dedup', partition_key)\n",
    "            \n",
    "            print df.count()\n",
    "            df.dropna(inplace=True)\n",
    "            print 'After dropna..'\n",
    "            print df.count()\n",
    "\n",
    "            df.to_csv(outfile, \\\n",
    "                      header=False,\\\n",
    "                      index=False,\\\n",
    "                      encoding=\"utf-8\", \\\n",
    "                      mode=\"w+\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print \"error while deduping files\"\n",
    "            print e\n",
    "            \n",
    "\n",
    "filenamepart = \"TAGS\"\n",
    "nodeorrel = \"rel\"\n",
    "partition_key = \"2016_2_7\"\n",
    "\n",
    "dedup_file_test(filenamepart, nodeorrel, partition_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
